# Sample Logstash configuration for creating a simple
# Beats -> Logstash -> Elasticsearch pipeline.

# 从kafka接受输入，并以json格式进行解析
input {
  kafka {
    codec => "json"
    topics => ["ms-elk-log"]
    # Kafka实例的url列表，默认 "localhost:9092"，支持逗号分隔 host1:port1,host2:port2
    bootstrap_servers => "10.10.102.142:9092"
    auto_offset_reset => "latest"
  }
}

# 对日志数据，经过过滤预处理
filter {
  if "ms-log" in [tags] {
    json {
      source => "message"
      target => "messages"
    }
	
    date {
      match => ["[messages][timestamp]","yyyy-MM-dd HH:mm:ss.SSS"]
      target => "@log_timestamp"
    }

    ruby {
      code => "
        path = event.get('log')['file']['path']
        if (!path.nil?) && (!path.empty?)
          event.set('app_index', path.split('/')[-1].split('.')[0])
        end
        mytid = event.get('messages')['tid']
        if (!mytid.nil?) && (!mytid.empty?)
          event.set('tid',mytid.split(':')[1])
        end
      "
    }	
	
    mutate{
      # 添加新字段
      add_field=>{
        'thread' => "%{[messages][thread]}"
        'level' => "%{[messages][level]}"
        'logger_name' => "%{[messages][logger_name]}"
        'message_info' => "%{[messages][message]}"
      }
      # 移除多余不需要的字段
      remove_field => ["agent","ecs","@version","host","messages"]
    }
  }
}

# 输出至elasticsearch
output {
  if "bufx-log" in [tags] {
    elasticsearch {
      hosts => ["10.10.102.142:9200"]
      index => "ms-log-%{+YYYY.MM.dd}"
      template => "/usr/share/logstash/pipeline/ms-template.json"
      template_name => "ms-log-template"
      template_overwrite => true
      # user => ""
      # password => ""
    }
  }
#  stdout{
#    codec => json
#  }
}
